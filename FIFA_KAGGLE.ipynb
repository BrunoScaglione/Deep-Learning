{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "FIFA_KAGGLE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aCHOORqMKB_n",
        "lvedt7kZKB_r",
        "j62_ncgaKB_8",
        "plF6eU1dKCAC",
        "jEZO382FKCAR",
        "xo2EyCe9KCAd",
        "oQzrlbFXKCAj",
        "Xh3MDsW3KCAn",
        "jLtuthupKCAt",
        "DW6nFTfgKCA0",
        "g3FgT_uSKCA_",
        "vdasrOmDKCBC",
        "joMdBMuGKCBD",
        "leKCEGGnKCBD",
        "ckJB0Ge4KCBE"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoScaglione/Deep-Learning/blob/master/FIFA_KAGGLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm9E-Sh6KB-r",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook walks through the whole process of machine learning with its different steps and provide web services and mobile applications to predict the player positions. The main goal is to apply these differenet steps to the [Fifa 19 data set][1].\n",
        "\n",
        "![Fifa19 DataSet](https://storage.googleapis.com/kaggle-datasets-images/73041/162580/a17bc456289e382192bebd8bb595719d/dataset-cover.jpg?t=2018-11-04-22 9-55)\n",
        "\n",
        "**Data Set**\n",
        "\n",
        "[Fifa 19][2] is the most famous and played soccer game around the whole world with over 1.2 billion players. The game contains more than 70000 players in their [databases][3]. \n",
        "\n",
        "The data set presented in this challenge contains more than 18000 players with their different features from physical appearence, clubs, wages and their performance. Our goal is to take those feature and predict the player position correctly.\n",
        "\n",
        "**Steps Applied**\n",
        "\n",
        "1. Understand the data using Google Facets\n",
        "   \n",
        "2. Prepocess the data\n",
        "\n",
        "3. Apply the [EDA][4](Exploratory Data Analysis) - Visualization\n",
        "\n",
        "4. Divide the data to train and test datasets\n",
        "\n",
        "5. Apply different models and algorithms and tune the hyper parameters\n",
        "\n",
        "    5.1. Logistic regression\n",
        "    \n",
        "    5.2. KNN\n",
        "    \n",
        "    5.3. Decision Tree\n",
        "    \n",
        "    5.4. SVM\n",
        "    \n",
        "    5.5. Neural Network\n",
        "\n",
        "6. Evaluate the models\n",
        "\n",
        "7. Apply AutoML on the data\n",
        "\n",
        "8. Features engineering\n",
        "\n",
        "9. Create Web service to apply the prediction\n",
        "\n",
        "[1]: https://www.kaggle.com/karangadiya/fifa19\n",
        "[2]: https://www.ea.com/games/fifa/fifa-19\n",
        "[3]: https://www.fifaindex.com/\n",
        "[4]: https://en.wikipedia.org/wiki/Exploratory_data_analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HtnWJHXKB-t",
        "colab_type": "text"
      },
      "source": [
        "# 1.Understand the data using Google Facets\n",
        "\n",
        "\n",
        "For this part, to understand a little bit about the data, such as general information and how many data are missing we used [Google Facets][1] a very powerful tool to Understed the data which is critical to build a powerful machine learning system.\n",
        "\n",
        "\n",
        "**Numerical Features**\n",
        "\n",
        "![Numerical Feature 1](https://i.ibb.co/N6DB4Tq/01-Numerical-Feature.png)\n",
        "\n",
        "![Numerical Feature 2](https://i.ibb.co/fG6wnsZ/02-Numerical-Feature.png)\n",
        "\n",
        "![Numerical Feature 3](https://i.ibb.co/RzhY43t/03-Numerical-Feature.png)\n",
        "\n",
        "We Observe that there is more than 18.2K players in the dataset. Some of the attributes are missing for some players such as the weak foot and the skills ratings. \n",
        "\n",
        "**Categorical Features**\n",
        "\n",
        "![Categorical Feature 1](https://i.ibb.co/MMPyzdT/04-Categoratical-Feature.png)\n",
        "\n",
        "![Categorical Feature 2](https://i.ibb.co/1MK9cxj/05-Categoratical-Feature.png)\n",
        "\n",
        "We Observe that less than 1% of the data doesn't have position and some others features. Position is the Y feature therefore  we could use it as test dataset or deleting it. \n",
        "\n",
        "**Grouping plotting**\n",
        "\n",
        "In This section we tried to plot the data according to different features. The legend represents the actual players positions.\n",
        "\n",
        "* Finishing per Age per Position\n",
        "\n",
        "![Finishing per Age per Position](https://i.ibb.co/vvZt49L/06-Analysis.png)\n",
        "\n",
        "We Observe that most of the high finishing skills are the strikers, which totally make sense.\n",
        "\n",
        "* Short Pass per Age per Position\n",
        "\n",
        "![Short Pass per Age per Position](https://i.ibb.co/THc1Yhf/07-Analysis.png)\n",
        "\n",
        "In the same way as the finishing, the high skilled players in the field of passing are the midlifers. \n",
        "\n",
        "\n",
        "\n",
        "[1]: https://pair-code.github.io/facets/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI6zFXFGKB-u",
        "colab_type": "text"
      },
      "source": [
        "# 2.Preprocess the data\n",
        "\n",
        "**Load Libraries**\n",
        "\n",
        "Now, we get to the coding part when we explore that data using python and anaconda libraries. First of all we load the libraires."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "HcsEx25UKB-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the libraries and imports\n",
        "# Panda\n",
        "import pandas as pd\n",
        "#mat plot\n",
        "import matplotlib.pyplot as plt\n",
        "#Sea born\n",
        "import seaborn as sns\n",
        "#Num py\n",
        "import numpy as np\n",
        "#Sk learn imports\n",
        "from sklearn import tree,preprocessing\n",
        "#ensembles\n",
        "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
        "import sklearn.metrics as metrics\n",
        "#scores\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,roc_auc_score,auc  \n",
        "#models\n",
        "from sklearn.model_selection import StratifiedKFold,train_test_split,cross_val_score,learning_curve,GridSearchCV,validation_curve\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "#export the model\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tLnw_UYKB-z",
        "colab_type": "text"
      },
      "source": [
        "We define a set of functions that would help us loading the data and preprocess some of the features that we are interested into.\n",
        "\n",
        "Note that we are only interested in this project in 3 players positions 'Strikers', 'Midfielders' and 'Defenders'. For more deep project we could have more classes such as specific positions saying 'Left back' or 'center forward'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "YUZCCA7gKB-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data from the path to the dataSet\n",
        "def load_dataset(dataSet_path):\n",
        "    data = pd.read_csv(dataSet_path)\n",
        "    return data\n",
        "\n",
        "#Imputation\n",
        "def impute_data(df):\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "# Coversion weight to int\n",
        "def weight_to_int(df):\n",
        "    df['Weight'] = df['Weight'].str[:-3]\n",
        "    df['Weight'] = df['Weight'].apply(lambda x: int(x))\n",
        "    return df\n",
        "\n",
        "# Coversion height to int\n",
        "def height_convert(df_height):\n",
        "        try:\n",
        "            feet = int(df_height[0])\n",
        "            dlm = df_height[-2]\n",
        "            if dlm == \"'\":\n",
        "                height = round((feet * 12 + int(df_height[-1])) * 2.54, 0)\n",
        "            elif dlm != \"'\":\n",
        "                height = round((feet * 12 + int(df_height[-2:])) * 2.54, 0)\n",
        "        except ValueError:\n",
        "            height = 0\n",
        "        return height\n",
        "\n",
        "def height_to_int(df):\n",
        "    df['Height'] = df['Height'].apply(height_convert)\n",
        "    \n",
        "#One Hot Encoding of a feature\n",
        "def one_hot_encoding(df,column):\n",
        "    encoder = preprocessing.LabelEncoder()\n",
        "    df[column] = encoder.fit_transform(df[column].values)\n",
        "        \n",
        "\n",
        "#Drop columns that we are not interested in\n",
        "def drop_columns(df):\n",
        "    df.drop(df.loc[:, 'Unnamed: 0':'Name' ],axis=1, inplace = True)\n",
        "    df.drop(df.loc[:, 'Photo':'Special'],axis=1, inplace = True)\n",
        "    df.drop(df.loc[:, 'International Reputation':'Real Face' ],axis=1, inplace = True)\n",
        "    df.drop(df.loc[:, 'Jersey Number':'Contract Valid Until' ],axis=1, inplace = True)\n",
        "    df.drop(df.loc[:, 'LS':'RB'],axis=1, inplace = True)\n",
        "    df.drop(df.loc[:, 'GKDiving':'Release Clause'],axis=1, inplace = True)\n",
        "\n",
        "#Transform positions to 3 categories 'Striker', 'Midfielder', 'Defender'    \n",
        "def transform_positions(df):\n",
        "    for i in ['ST', 'CF', 'LF', 'LS', 'LW', 'RF', 'RS', 'RW']:\n",
        "      df.loc[df.Position == i , 'Position'] = 'Striker' \n",
        "    \n",
        "    for i in ['CAM', 'CDM', 'LCM', 'CM', 'LAM', 'LDM', 'LM', 'RAM', 'RCM', 'RDM', 'RM']:\n",
        "      df.loc[df.Position == i , 'Position'] = 'Midfielder' \n",
        "    \n",
        "    for i in ['CB', 'LB', 'LCB', 'LWB', 'RB', 'RCB', 'RWB','GK']:\n",
        "      df.loc[df.Position == i , 'Position'] = 'Defender' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwu8jPr9KB-3",
        "colab_type": "text"
      },
      "source": [
        "Let's apply these functions to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wgqoKmJIKB-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset\n",
        "df= load_dataset(\"../input/data.csv\")\n",
        "# Drop columns that we are not interested in\n",
        "drop_columns(df)\n",
        "# Impute the data that is null\n",
        "impute_data(df)\n",
        "# transform weight and height to integer values\n",
        "weight_to_int(df)\n",
        "height_to_int(df)\n",
        "# apply the one hot encoding to the Preferred foot (L,R) => (0,1)\n",
        "one_hot_encoding(df,'Preferred Foot')\n",
        "# transform position to striker, midfielder, defender\n",
        "transform_positions(df)\n",
        "# show the 10 first rows\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NVftu_XKB-7",
        "colab_type": "text"
      },
      "source": [
        "Let's see the features we will be working on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CGhBmj2QKB-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri7royUtKB--",
        "colab_type": "text"
      },
      "source": [
        "# 3.Visualization\n",
        "\n",
        "In this section, we will apply the EDA to visualize the data with several plottings.\n",
        "\n",
        "**Players counts by position**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uOOXOk3WKB-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count number of players in each position using countplot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"Number of Players by position\")\n",
        "fig = sns.countplot(x = 'Position', data =df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQwCEfAIKB_B",
        "colab_type": "text"
      },
      "source": [
        "We notice that most of the players are defenders and midfielders which makes sense, since that in every team we need less strikers.\n",
        "\n",
        "Now, we want to explore the players skills features to plot them as catagories, but these are numerical. Therefor, we will first transform them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rcbqPKBvKB_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define categorical skills base on the rating\n",
        "def categorize_skill(df,column):\n",
        "    bins = (10,30,50,70,100)\n",
        "    group_names = ['Low','Moderate','High','VeryHigh']\n",
        "    categories = pd.cut(df[column],bins,labels=group_names)\n",
        "    new_column = column+'_cat'\n",
        "    df[new_column]=categories\n",
        "categorize_skill(df,\"Finishing\")\n",
        "categorize_skill(df,\"Strength\")\n",
        "categorize_skill(df,\"FKAccuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6xDqAmpKB_F",
        "colab_type": "text"
      },
      "source": [
        "Let's make some plots.\n",
        "\n",
        "**Short Passing by finishing according to the positions**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "p6BZuagXKB_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crate Category plot from seaborn on Finishing & ShortPassing By position\n",
        "sns.catplot(x=\"Finishing_cat\", y=\"ShortPassing\", hue=\"Position\",\n",
        "            markers=[\"^\", \"o\",\"x\"], linestyles=[\"-\", \"--\",\"-\"],\n",
        "            kind=\"point\", data=df);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0KNaxbNKB_M",
        "colab_type": "text"
      },
      "source": [
        "**Interceptions by Strength according to the positions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RRcueQdbKB_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crate Category plot from seaborn on  Strength & Interception By position\n",
        "sns.catplot(x=\"Strength_cat\", y=\"Interceptions\", hue=\"Position\",\n",
        "            markers=[\"^\", \"o\",\"x\"], linestyles=[\"-\", \"--\",\"-\"],\n",
        "            kind=\"point\", data=df);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpxcoG_fKB_Q",
        "colab_type": "text"
      },
      "source": [
        "**Interceptions by Strength according to the positions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "g9Bhyn10KB_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crate Category plot from seaborn on FKAccuracy & Penalties By position\n",
        "sns.catplot(x=\"FKAccuracy_cat\", y=\"Penalties\", hue=\"Position\",\n",
        "            markers=[\"^\", \"o\",\"x\"], linestyles=[\"-\", \"--\",\"-\"],\n",
        "            kind=\"point\", data=df);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_Ep_K29KB_T",
        "colab_type": "text"
      },
      "source": [
        "**Physical appearences by position**\n",
        "\n",
        "Let's plot some physical appearences such as age, height and weight and see how it will affect the players positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Z9I_o_NpKB_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Box plot skills by position\n",
        "f, axes = plt.subplots(2, 2, figsize=(15, 15), sharex=False)\n",
        "sns.despine(left=True)\n",
        "sns.boxplot('Position', 'Jumping', data = df, ax=axes[0, 0])\n",
        "sns.boxplot('Position', 'Age', data = df, ax=axes[0, 1])\n",
        "sns.boxplot('Position', 'Height', data = df, ax=axes[1, 0])\n",
        "sns.boxplot('Position', 'Weight', data = df, ax=axes[1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJLU9zteKB_Y",
        "colab_type": "text"
      },
      "source": [
        "**Reaction by age**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Nq_B4PfkKB_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bar plot Reaction by Age\n",
        "mean_value_per_age = df.groupby('Age')['Reactions'].mean()\n",
        "p = sns.barplot(x = mean_value_per_age.index, y = mean_value_per_age.values)\n",
        "p = plt.xticks(rotation=90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7oVnmf5KB_b",
        "colab_type": "text"
      },
      "source": [
        "**Scatter plot skills and positions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kDIGOy4nKB_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Scatter plot Finishing by shortPassing classified by position\n",
        "ax = sns.scatterplot(x=\"ShortPassing\", y=\"Finishing\", hue=\"Position\",data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR3th7SQKB_g",
        "colab_type": "text"
      },
      "source": [
        "**Features correlation**\n",
        "\n",
        "Let's correlate our features, but before that let's drop the categorical columns that we have created for plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tR0JM9rWKB_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop some of unuseful coloumns\n",
        "drop_elements = ['Position', 'Finishing_cat', 'Strength_cat', 'FKAccuracy_cat']\n",
        "train=df.drop(drop_elements, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45WvuzYbKB_j",
        "colab_type": "text"
      },
      "source": [
        "Now, let's plot all the numerical features for correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hIO-8QZcKB_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the heat map of features correlation\n",
        "colormap = plt.cm.RdBu\n",
        "plt.figure(figsize=(14,12))\n",
        "plt.title('Correlation of Features', y=1.05, size=15)\n",
        "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n",
        "            square=True, cmap=colormap, linecolor='white', annot=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOOa2K9zKB_m",
        "colab_type": "text"
      },
      "source": [
        "We see that some of the features are very correlated, this could be very helpful when we will be doing the features engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCHOORqMKB_n",
        "colab_type": "text"
      },
      "source": [
        "# 4.Divide the data to train and test datasets\n",
        "\n",
        "Our test size would be 20% from the data, the most recommended into the field. We will have then 80% as training data and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qpETYZgmKB_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Divide the data to train and test\n",
        "\n",
        "# Drop the elements that has been created for \n",
        "drop_elements = ['Finishing_cat', 'Strength_cat', 'FKAccuracy_cat']\n",
        "df=df.drop(drop_elements, axis = 1)\n",
        "\n",
        "# Create the unique values for the positions encoded as Defender:0, Midfielder:1, Striker:2\n",
        "positions = df[\"Position\"].unique()\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "df['Position'] = encoder.fit_transform(df['Position'])\n",
        "\n",
        "#The Y feature is the position\n",
        "y = df[\"Position\"]\n",
        "\n",
        "#The other features are all but the position\n",
        "df.drop(columns=[\"Position\"],inplace=True)\n",
        "\n",
        "#Split the data\n",
        "X_train_dev, X_test, y_train_dev, y_test = train_test_split(df, y, \n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=42 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvedt7kZKB_r",
        "colab_type": "text"
      },
      "source": [
        "# 5.Apply different models and algorithms and tune the hyper parameters\n",
        "\n",
        "In this section we will train different models, find their score performance, tune the hyper parameters.\n",
        "\n",
        "First, we will define some functions that will help us train the models, evaluate and plot the curves.\n",
        "\n",
        "**Plot Confusion Matrix**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "t52Qwg5wKB_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the confusion matrix\n",
        "def plot_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
        "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    confusion_matrix: numpy.ndarray\n",
        "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
        "        Similarly constructed ndarrays can also be used.\n",
        "    class_names: list\n",
        "        An ordered list of class names, in the order they index the given confusion matrix.\n",
        "    figsize: tuple\n",
        "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
        "        the second determining the vertical size. Defaults to (10,7).\n",
        "    fontsize: int\n",
        "        Font size for axes labels. Defaults to 14.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.figure.Figure\n",
        "        The resulting confusion matrix figure\n",
        "    \"\"\"\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names, \n",
        "    )\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    sns.set(font_scale=1.4)\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", annot_kws={\"size\": 16})\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsddvgkuKB_y",
        "colab_type": "text"
      },
      "source": [
        "**Plot Learning Curve**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "u09f8j77KB_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 3-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSYuaNRZKB_1",
        "colab_type": "text"
      },
      "source": [
        "**Plot validation curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hhzsehfuKB_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_curve(ticks, train_scores, test_scores):\n",
        "    train_scores_mean = -1 * np.mean(train_scores, axis=1)\n",
        "    train_scores_std = -1 * np.std(train_scores, axis=1)\n",
        "    test_scores_mean = -1 * np.mean(test_scores, axis=1)\n",
        "    test_scores_std = -1 * np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.fill_between(ticks, \n",
        "                     train_scores_mean - train_scores_std, \n",
        "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"b\")\n",
        "    plt.fill_between(ticks, \n",
        "                     test_scores_mean - test_scores_std, \n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"r\")\n",
        "    plt.plot(ticks, train_scores_mean, 'b-', label='Training Error')\n",
        "    plt.plot(ticks, test_scores_mean, 'r-', label='Validation Error')\n",
        "    plt.legend(fancybox=True, facecolor='w')\n",
        "\n",
        "    return plt.gca()\n",
        "\n",
        "def plot_validation_curve(clf, X, y, param_name, param_range, scoring='accuracy'):\n",
        "    plt.xkcd()\n",
        "    ax = plot_curve(param_range, *validation_curve(clf, X, y, cv=4, \n",
        "                                                   scoring=scoring, \n",
        "                                                   param_name=param_name, \n",
        "                                                   param_range=param_range, n_jobs=4))\n",
        "    ax.set_title('')\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_xlim(2,12)\n",
        "    ax.set_ylim(-0.97, -0.83)\n",
        "    ax.set_ylabel('Error')\n",
        "    ax.set_xlabel('Model complexity')\n",
        "    ax.text(9, -0.94, 'Overfitting', fontsize=14)\n",
        "    ax.text(3, -0.94, 'Underfitting', fontsize=14)\n",
        "    ax.axvline(7, ls='--')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZDxftzGKB_4",
        "colab_type": "text"
      },
      "source": [
        "**Train the model and score**\n",
        "\n",
        "This function is responisble for traing the model and scoring it. We print at the end the Accuracy and F1 Score metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jhKtja0MKB_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_score(clf,X_train,y_train,X_test,y_test):\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_test)\n",
        "    cf = confusion_matrix(y_test,preds)\n",
        "\n",
        "    print(plot_confusion_matrix(cf, class_names=positions))\n",
        "\n",
        "    print(\" Accuracy: \",accuracy_score(y_test, preds))\n",
        "    print(\" F1 score: \",metrics.f1_score(y_test, preds,average='weighted'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j62_ncgaKB_8",
        "colab_type": "text"
      },
      "source": [
        "# 5.1. Logistic regression\n",
        "\n",
        "We apply our first model Logistic regression with a cross validation of 5 folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8OqQyBGEKB_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = LogisticRegressionCV(cv=5,random_state=20, solver='lbfgs',\n",
        "                             multi_class='multinomial')\n",
        "train_and_score(LR,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4_Xk19pmKCAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(LR, \"Logistic Regression Curve\", X_train_dev, y_train_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plF6eU1dKCAC",
        "colab_type": "text"
      },
      "source": [
        "# 5.2. K-nearest Neighbours\n",
        "\n",
        "Then, we apply the KNN model with gread serach that apply to the K (number of neighbors) in a range from 1 to 25 with a cross validation of 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GmAxQ1IEKCAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create new a knn model\n",
        "knn_model = KNeighborsClassifier()\n",
        "#create a dictionary of all values we want to test for n_neighbors\n",
        "param_grid = {'n_neighbors': np.arange(1, 25)}\n",
        "#use gridsearch to test all values for n_neighbors\n",
        "KNN = GridSearchCV(knn_model, param_grid, cv=5)\n",
        "\n",
        "train_and_score(KNN,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "801WwwNlKCAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(KNN, \"KNN Regression Curve\", X_train_dev, y_train_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-2OjhfHKCAK",
        "colab_type": "text"
      },
      "source": [
        "We want to draw the model complexity curve and see how far the model is learning based on the n_neighbours range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "maSzSODKKCAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_validation_curve(KNeighborsClassifier(), X_train_dev, y_train_dev, param_name='n_neighbors', param_range=range(2,25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEZO382FKCAR",
        "colab_type": "text"
      },
      "source": [
        "# 5.3. Decision Tree\n",
        "\n",
        "First all, let's define a function that calculate the best minimum impurity a parameter to pass to the decision tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DeaseX45KCAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_impurity(X,y):\n",
        "    tr_acc = []\n",
        "    mln_set = range(75,90)                                 \n",
        "\n",
        "    for minImp in mln_set:\n",
        "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\",min_impurity_decrease=minImp/100000)\n",
        "        scores = cross_val_score(clf, X, y, cv=10)\n",
        "        tr_acc.append(scores.mean())\n",
        "\n",
        "    best_mln = mln_set[np.argmax(tr_acc)]\n",
        "    return best_mln\n",
        "\n",
        "best_min= min_impurity(X_train_dev,y_train_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfnKq_MhKCAV",
        "colab_type": "text"
      },
      "source": [
        "Now, let's apply the Decision Tree model with the best minimum impurity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gPcbiAjAKCAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DT = tree.DecisionTreeClassifier(criterion=\"entropy\",min_impurity_decrease=best_min/100000)\n",
        "train_and_score(DT,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l-mBHb6sKCAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(DT, \"Decision Tree Learning Curve\", X_train_dev, y_train_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo2EyCe9KCAd",
        "colab_type": "text"
      },
      "source": [
        "# 5.3.1. Bagging\n",
        "\n",
        "Now, let's work with DT but using the bagging model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wUbcGx56KCAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DTBG = BaggingClassifier(tree.DecisionTreeClassifier(criterion=\"entropy\",min_impurity_decrease=best_min/100000))#\n",
        "train_and_score(DTBG,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hwpbciT3KCAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(DTBG, \"Bagging Decision Tree Learning Curve\", X_train_dev, y_train_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQzrlbFXKCAj",
        "colab_type": "text"
      },
      "source": [
        "# 5.3.2. Boosting\n",
        "\n",
        "Now, let's try another ensembling technique the boosting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QfN1AlGzKCAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtrain = xgb.DMatrix(X_train_dev, label=y_train_dev)\n",
        "\n",
        "dtest = xgb.DMatrix(X_test,label=y_test)\n",
        "\n",
        "param = {\n",
        "    'max_depth': 3,  # the maximum depth of each tree\n",
        "    'eta': 0.3,  # the training step for each iteration\n",
        "    'silent': 1,  # logging mode - quiet\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
        "    'num_class': 3}  # the number of classes that exist in this datset\n",
        "num_round = 50  # the number of training iterations\n",
        "DTBST = xgb.train(param, dtrain, num_round)\n",
        "DTBST.dump_model('dump.raw.txt')\n",
        "preds = DTBST.predict(dtest)\n",
        "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
        "cf = confusion_matrix(y_test, best_preds)\n",
        "\n",
        "print(plot_confusion_matrix(cf, class_names=positions))\n",
        "print(\" Accuracy: \",accuracy_score(y_test, best_preds))\n",
        "print(\" F1 score: \",metrics.f1_score(y_test, best_preds,average='weighted'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh3MDsW3KCAn",
        "colab_type": "text"
      },
      "source": [
        "# 5.3.3. Random Forest\n",
        "\n",
        "Now, let's try another ensembling technique the random forest that would search to tune many parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rq3iT43cKCAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gridsearch_forest = RandomForestClassifier()\n",
        "\n",
        "params = {\n",
        "    \"n_estimators\": [1, 10, 100],\n",
        "    \"max_depth\": [5,8,15], #2,3,5 85 #5,8,10 88 #5 8 15 89\n",
        "    \"min_samples_leaf\" : [1, 2, 4]\n",
        "}\n",
        "\n",
        "RF = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 )\n",
        "train_and_score(RF,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dz9ov1vQKCAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(RF, \"Random Forest Learning Curve\", X_train_dev, y_train_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLtuthupKCAt",
        "colab_type": "text"
      },
      "source": [
        "# 5.5. SVM\n",
        "\n",
        "In this section we will be using the neural network with different parameters of the number of layers and the size. After tuning the parameters we find that the best option is 50-20 NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QqiVbjzHKCAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SVM = SVC(kernel='linear', C=1)\n",
        "train_and_score(SVM,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OoOei8XDKCAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(SVM, \"SVM Curve\", X_train_dev, y_train_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW6nFTfgKCA0",
        "colab_type": "text"
      },
      "source": [
        "# 5.5. Neural Network\n",
        "\n",
        "In this section we will be using the neural network with different parameters of the number of layers and the size. After tuning the parameters we find that the best option is 50-20 NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "npeby4OoKCA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
        "train_and_score(MLP,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "N64yN1tSKCA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                    hidden_layer_sizes=(10, 5), random_state=1)\n",
        "train_and_score(MLP,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WSDwdl-zKCA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                    hidden_layer_sizes=(20, 15), random_state=1)\n",
        "train_and_score(MLP,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ag9Kms6wKCA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                    hidden_layer_sizes=(50, 20), random_state=1)\n",
        "train_and_score(MLP,X_train_dev,y_train_dev,X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fG5MCochKCA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(MLP, \"Neural Network Curve\", X_train_dev, y_train_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3FgT_uSKCA_",
        "colab_type": "text"
      },
      "source": [
        "# 6. Evaluate the models\n",
        "\n",
        "According to the accuracy we can see that almost all of the models get about the same score. The best model is the Neural Network with 50,20 parameters.\n",
        "\n",
        "Now let's draw the ROC-AUC curve and plot the different models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TM_kdn5rKCBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "plt.figure()\n",
        "\n",
        "y1_test=y_test[(y_test ==0) | (y_test ==1)]\n",
        "x1_test = X_test[X_test.index.isin(y1_test.index)]\n",
        "\n",
        "\n",
        "y_predict_probabilities = LR.predict_proba(x1_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y1_test, y_predict_probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=2, label='LR (area = %0.3f)' % roc_auc)\n",
        "\n",
        "y_predict_probabilities = KNN.predict_proba(x1_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y1_test, y_predict_probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='blue',\n",
        "         lw=2, label='KNN (area = %0.3f)' % roc_auc)\n",
        "\n",
        "y_predict_probabilities = RF.predict_proba(x1_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y1_test, y_predict_probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='green',\n",
        "         lw=2, label='RF (area = %0.3f)' % roc_auc)\n",
        "\n",
        "y_predict_probabilities = MLP.predict_proba(x1_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y1_test, y_predict_probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='red',\n",
        "         lw=2, label='NN (area = %0.3f)' % roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r94Bc9Q0KCBC",
        "colab_type": "text"
      },
      "source": [
        "Note that all of them are so close."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdasrOmDKCBC",
        "colab_type": "text"
      },
      "source": [
        "# 7. AutoML\n",
        "We applied AutoML from SKLearn on the data trying to find the best estimator. We run AutoML on a dedicated server with some capabilities and here we present the code and the output.\n",
        "\n",
        "![AutoML_Code](https://i.ibb.co/PrWWTrz/AutoML-1.png)\n",
        "\n",
        "After one hour of code running, we got finnaly the results.\n",
        "\n",
        "![AutoML_Result](https://i.ibb.co/vPGXLRx/AutoML-2.png)\n",
        "\n",
        "We see that the best accuracy is : 0.895 which is so close to the Neural Network of 50,20.\n",
        "\n",
        "Finnaly, we looked for the best estimator.\n",
        "\n",
        "![AutoML_Result](https://i.ibb.co/K6t45hf/AutoML-3.png)\n",
        "\n",
        "We found out that the best estimator accoring to AutoML is the gradient boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joMdBMuGKCBD",
        "colab_type": "text"
      },
      "source": [
        "# 8. Feature engineering\n",
        "\n",
        "After comparing the different models we found that the Neural Network is the best model with the highest accuracy. We tried to change our features by deleting correlated features or gathering some features together, but this didn't make the model more accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leKCEGGnKCBD",
        "colab_type": "text"
      },
      "source": [
        "# 9. Web Services\n",
        "\n",
        "Finnaly, we create a web service that throw a post with the data to predict that could be found on the following link : http://thefourtwofour.com:1992/api/predict\n",
        "\n",
        "Here an example of a post request and the results :\n",
        "![Post Predict](https://i.ibb.co/RpkBYQc/Post.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckJB0Ge4KCBE",
        "colab_type": "text"
      },
      "source": [
        "# Refrences\n",
        "\n",
        "This notebook has been produced using some external kernles and ressources listed below: \n",
        "\n",
        "[Fifa 19 Kernel][1] \n",
        "\n",
        "[SK Learn][2] \n",
        "\n",
        "\n",
        "[1]: https://www.kaggle.com/brunosette/classification-of-field-position-decision-tree/data?fbclid=IwAR1_gS-pS9qXcyqEtRKZvmPXlwjwbK2Fd88NV_9OsayaE8LI44LTpoKJTVQ\n",
        "[2]: https://scikit-learn.org/stable/"
      ]
    }
  ]
}